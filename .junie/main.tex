\documentclass[journal]{IEEEtran}

\usepackage[english]{babel}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{bigstrut}
\usepackage{array}
\usepackage{makecell}
\usepackage{listings}
\usepackage{amssymb}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\title{\textbf{BPoSt}: Fully Public Proof of Storage-time Based on Blockchain}

\author{(Anonymous Authors)}
\date{2025}

\begin{document}

\maketitle

\begin{abstract}
    
    In recent years, the tremendous growth of data has given rise to a new wave of distributed storage. Increasingly, enterprises and individuals are opting to utilize distributed storage to host their data; however, traditional distributed storage services often encounter security and performance issues. Especially in the environment of a public network, issues such as how to store data reliably, how to verify whether the storage is qualified, and how to minimize the overhead of data preprocessing for users are all problems that need to be considered. In this paper, we start from blockchain technology and aim to propose a solution to the above problems of decentralized data storage in the permissionless network. This article basically relies on blockchain's public verifiability, combines a variant of Provable Data Possession and incrementally verifiable computation, and gives a client-friendly, fair storage side, public verifiability, and check-allowing at any time of the PoSt consensus protocol. With formal methods and actual model evaluation, we also obtained some actual performances of this protocol.
\end{abstract}

\begin{IEEEkeywords}
Consensus Protocol, Proof of Storage-time, Zero Knowledge Proof, Provable data possession, incrementally verifiable computation, Decentralized storage network.
\end{IEEEkeywords}

\section{Introduction}

With the increase in personal data needs, more and more storage needs are being put forward. The user, as a data user, often puts more attention on the ability to calculate, that do not have a lot of storage space to store the increasing amount of data, leading to the necessity of distributed storage. However, for some situations, more specialized storage services are needed to achieve some strict needs. For example, how to ensure the correct storage of data when the data volume is too large? How to ensure whether the storage is valid when the network environment is untrustworthy? And how to reduce the huge cost caused by the storage behavior to the user? Some research in distributed storage has provided some of the answers to these questions. We will give an effective solution mainly from the perspective of blockchain technology, as a consensus protocol to coordinate the participating parties to achieve decentralized, publicly verifiable storage, which we call Blockchain-based Proof of Storage-time (BPoSt).


For the blockchain, its most essential feature is public, where anyone on the network can submit their efforts under the premise that the operation is legal; on the other hand, anyone on the network can also audit the blocks through consensus. Distributed storage on the permissionless network needs this public verifiable feature, especially in terms of the reliability of storing a large amount of data. Only by storing data continuously without any interruption or cheating can the \textit{employer} willingly pay the \textit{salary}. Previous articles on Proof of Storage-time have focused on providing a solution without considering the problem from the perspective of consensus protocols, but our article not only gives an effective solution, but also condenses the solution into a consensus protocol, so that it can be better utilized on the blockchain.


Before the emergence of PoSt, the understanding of storage was limited to "hold it here and now", e.g., Provable Data Possession\cite{2007pdp}, which utilizes the challenge-proof model to prove that the server actually held the data at the moment of the challenge. Another case is proofs of Retrievability\cite{2007pors}, which achieves provable recovery through special encoding and inclusion of checksum blocks during data preprocessing. All of these can be called Proof of Space, but when talking about continuous storage validation, then manually and frequently initiated challenges by users cannot be avoided, which definitely causes huge costs and leads to the necessity of PoSt.


Proof of Storage-time was originally derived from the article \cite{2019pospacetime} by Moran et al. They proposed a method that could be used by verifiers to verify that the prover did indeed use the amount of time and space to store the amount of data, which offers the precedent for similar ideas that followed. In order to alleviate the reliance of existing cryptocurrencies on Proof of Work(PoW)\cite{2008bitcoin} in current research, they focused their research on finding alternatives to PoW, proposed Proof of Spacetime \footnote{The name of Proof of Spacetime has been changed into Proof of Storage-time by \cite{2020cpost} to avoid confusion. We use the latter one in this article.}, i.e., storing a specific storage space for a specific time, as an alternative to CPU workload in PoW, which has an excellent performance under certain attacks, and also offers an alternative solution to the high energy overhead of PoW, and provides an alternative solution to PoS\cite{2013pos}-based cryptocurrency Spacemint\cite{2018spacemint}. However, the solution in this paper is only at the prototype stage, and does not provide comprehensive answers to obvious questions, such as practical implementation or performance limitations.


Following the pioneering research of Moran et al, more research has given some specific ideas to address some of the potential problems in the original scheme. For example, in the Filecoin project \cite{2020filecoin}, they instantiated a PoSt scheme in a real-world environment, gave a way for PoSt to be used in real cryptocurrencies, and noticed the huge cost of an overly large proof chain for the verifier, and thus trying to use a ZK-SNARK technique \cite{2012zksnark, 2016groth16} to compress the proof chain. Later, Ateniese et al. formalized the definition of PoSt and proposed the provably secure cPoSt\cite{2020cpost}, which changed the original purpose of "alternate" into solving the problem of continuous availability of data. Their study achieves the goal of provable security by introducing the TDF\cite{2018vdf} as a measure of time, which is a non-accelerating function that can be verified through trapdoors, and by formally defining the entire protocol. However, the protocol is stateful, the verification step requires user preprocessing as a prerequisite, and the defined cPoSt is not publicly verifiable, which is what a usable PoSt protocol should address.


With the deepening of the researches, researchers focus on facilitating the user (i.e., the storage demand side), the traditional PoSt requires the user to participate in the construction of the subsequent chain of proofs, in short, it is to prepare the "answers" for the "test questions" in advance, so that the "test" can be checked after the end of the "examination". However, this approach does not promote the popularization of this storage method, and will divide the performance bottleneck to the weak party of the performance, which, contrary to Moran's concept of \textit{Rational Storage}\cite{2019pospacetime}, so reducing the burden on the user side is necessary. Thus, Zhang et al. proposed a client-friendly PoSt -- ePoSt\cite{2023epost}, by using VDF and Verifiable Computation, their scheme can achieve that the storage node side completes the whole proof chain at the premise that the user side only offers the initial parameters.

 
\subsection{Basic Idea}
Based on previous research, our study optimizes and extends PoSt into a complete consensus protocol by combining the blockchain realities and specifically dividing the storage and time dimensions. In detail, through the extension of PDP\cite{2007pdp}, we obtain a distributed PDP scheme that is more suitable for blockchain networks, which can realize the public verifiability of single proofs, thus conforming to the public feature of blockchain, and this component provides us with the storage part of PoSt.


Further, we utilize the immutable and linear feature of blocks as our time anchor, and measure the storage duration by block height(e.g. $B_{StorageEnd} - B_{StorageBegin}$). However, the traditional way of block producing, i.e., measuring whether a block is ready to be released only by the computation result of a single node, will result in too much variance in the blocking interval, which is unfair to the storage node. For example, suppose node A and node B store the same size of data for the same block height for different time periods, but node A's storage periods' block generation speed is faster on average, while node B's is slower, which results in node B storing for a longer period of time on objective time scale, but in order to achieve the same benefit, node B has to compromise. So we change the block produce condition to a range average that reaches a threshold\cite{2020bobtail}.


As a distributed ledger, the block can often contain a lot of data. Using this feature, the storage state of each node can be added to the block, and further, to achieve public verifiability and irreversibility. At the same time, the Merkle tree in the classic PoW\cite {2008bitcoin} is used to store the relevant details, and this series of modifications is made on the basis of blockchain technology. Finally, we get a chain of logical proofs in a series of blocks. However, it is obvious that this chain is not user-friendly for verification, and users need to traverse the blocks to find their own proof chain. Therefore, we use the folding scheme \cite{2022nova} to allow the storage nodes to generate short proofs for user verification. So here is what we're aiming for:


\textit{Designing a Publicly Verifiable, Readily Checkable, User-Friendly, and provably secure Proof of Storage-Time Consensus Protocol.}


\subsection{Contribution}
In this study, we mainly construct a consensus protocol based on the PoSt concept, which achieves good performance, and formalize its security. The details go as follows.
\begin{itemize}
    \item 
    Our proposed BPoSt adapts the blockchain network on the basis of public verifiability and utilizes its public feature to further enhance the convenience of public verification of the protocol.
    \item 
    BPoSt somewhat overcomes the unfairness introduced by the variance of the block intervals by measuring the time dimension at a pace more similar to objective time.
    \item 
    BPoSt empowers users to initiate checks at any time, enabling faster detection of problems to a certain extent.
\end{itemize}


\subsection{Notations}


\section{Preliminary}
A complete protocol often requires multiple components to realize the corresponding functions. This part will start from the basic components of BPoSt, to lay the foundation for the construction of the protocol later. The core of the protocol can be logically divided into three parts: \textit{proof, consensus}, and \textit{ aggregation}. \textbf{Proof} refers to the round-by-round proof of a storage node's holdings of a particular piece of data, \textbf{Consensus} refers to the process of publicly uploading the relevant information of the current round, and \textbf{Aggregation} refers to the short aggregated proofs of the entire storage cycle generated by storage nodes at the end of the cycle, to simplify the process of users validation.


\subsection{Distributed Provable Data Possession}
To better utilize the advantages of the blockchain network in terms of public, we propose a Distributed Provable Data Possession scheme, where challenges can be computed by any node in the blockchain network (including storage nodes) using public parameters and the hash value of the last block on the blockchain without having to be released by a trusted third party as is the case with the existing PDP scheme\cite{2007pdp, 2013compact}which is published by a trusted third party.


\subsubsection{Roles}
In this scheme and in the subsequent parts of this study, we categorize the roles of protocol participants into two types: \textit{file owner} and \textit{storage node}.
\begin{itemize}
    \item \textbf{File owner: }
    Due to limited storage space, the file owner wants to store some large files in other nodes. In a distributed PDP scheme, he needs to do some preprocessing work on the files, such as chunking and initial signing of the files for subsequent verification.
    \item \textbf{Storage node: }
    These nodes contribute their remaining storage space to store files for other nodes and gain from this process (similar to competing for the reward of a block in Bitcoin). In each new block generation round, storage nodes can generate their own proofs of storage and compete for the subsequent leader. If they are selected as a leader in the current round, they need to complete the process of verifying and integrating the information of multiple selected nodes, or else skip the current round or submit their own block information to the leader as a selected node.
\end{itemize}


\subsubsection{Definitions of PDP}
As defined in \cite{2007pdp}, a PDP scheme is a collection of four polynomial time algorithms ($\text{KeyGen}$, $\text{TagFile}$, $\text{GenProof}$ ,and $\text{CheckProof}$) as follows:
\begin{itemize}
    \item $\text{KeyGen}(1^{k})\rightarrow(pk,sk)$ 
    is a probabilistic key generation algorithm run by the file owner. It takes as input a security parameter k and outputs a pair of public and private keys $(pk,sk)$.
    \item $\text{TagFile}(pk,sk,f)\rightarrow T_{f}$ 
    is an algorithm that is run by the owner of a file to generate validation metadata. It takes as input a public key pk, a private key sk, and a file f and outputs a collection of validation metadata $T_{f}$.
    \item $\text{GenProof}(pk,F,chal,V)\rightarrow pf$ 
    is an algorithm that is run by a storage node to create holding proofs. It takes as input a public key pk, an ordered set F of file blocks, a challenge chal, and an ordered set V of validation metadata corresponding to the blocks in F, and outputs a proof of holding pf for F.
    \item $\text{CheckProof}(pk,chal,pf)\rightarrow(0,1)$ 
    is run by any node in the blockchain system to publicly verify a holding proof. It takes as input a public key pk, a challenge chal, and a holding proof pf, and outputs one if the holding proof is correct and zero otherwise.
\end{itemize}


\begin{figure}[htbp]
    \centering
    \fbox{
        \begin{minipage}[c][8cm][c]{0.9\columnwidth}
            \centering
            
            \vspace*{\fill}
            \textit{PDP}
            \vspace*{\fill}
        \end{minipage}
    }
    \caption{Distributed PDP Scheme}
    \label{fig:PDP} 
\end{figure}


\subsubsection{Distributed PDP Scheme}
Based on the original PDP\cite{2007pdp} scheme, we divide our Distributed PDP into three basic units and make it have the ability to open a challenge based on additional related operations. The details are as follows.


\textbf{Setup:} 
The file owner generates public parameters, public-private key pairs, and metadata related to the file f.
\begin{itemize}
    \item 
    The file owner first runs $(pk,sk)\leftarrow \text{KenGen}(1^{k})$. He chooses a bilinear mapping $e:\mathbb{G}\times\mathbb{G}\rightarrow\mathbb{G}_{T}$, with $g$ and $u$ being the generating elements of $\mathbb{G}$. He chooses the private key $\alpha\in\mathbb{Z}_{p}$ and sets the public key to $(e,g,\beta,u,H_{1})$, where $\beta=g^{\alpha}$ and $H_{1}:\mathbb{Z}\rightarrow\mathbb{Z}_{p}$.
    \item 
    The file owner then runs $T_{f}\leftarrow \text{TagFile}(pk,sk,f)$. He divides the file $f$ into $m$ blocks: $b_{1},b_{2},\cdot\cdot\cdot,b_{m}\in\mathbb{Z}_{p}$ and computes the signature $\sigma_{i}$ for each block $b_i$:
    \begin{equation}
    \sigma_{i}=[H_{1}(i)\cdot u^{b_{i}}]^{\alpha}
    \end{equation}
    \item 
    The file owner chooses a number $a$, where $a$ is an approximation of $m$, and publishes the public key $pk$, the public parameter $(m, a)$, and the set of signatures $T_{f}=\{\sigma_{i}\}_{1\le i\le m}$ of the file $f$ and the set of file blocks $B=\{b_{i}\}_{1\le i\le m}$ to the blockchain network. He can then delete the file $f$ from his local storage.
\end{itemize}


\textbf{Proof generation:} 
At this step, the challenge may be computed by any node in the blockchain system using public parameters and a hash of the contents of the last block. The storage node computes and broadcasts the storage proof based on the challenge.
\begin{itemize}
    \item 
    Let $r$ be the current height of the blockchain, and any node in the blockchain, including the storage node itself, can compute the challenge for the current round $chal=\{i,v_{i}\}$, where $i\equiv1+H_{1}(Block_{r}) \pmod{m}$ and $v_{i}=H_{1}(i||Block_{r})$.
    \item 
    Storage node runs $pf \leftarrow \text{GenProof}(pk,F,chal,V)$ and generates the aggregated blocks and signatures as proofs of the storage file $f$ :
    \begin{equation}
    \begin{cases}\mu&=\sum_{(i,v_{i})\in chal}v_{i}\cdot b_{i}\\ \sigma&=\prod_{(i,v_{i})\in chal}\sigma_{i}^{v_{i}}\end{cases}
    \end{equation}
    \item 
    The storage node makes $pf=(\mu,\sigma)$ public via the blockchain network.
\end{itemize}


\textbf{Verification:} 
Any node in the blockchain network can verify the proof using the public key and public parameters.
\begin{itemize}
    \item 
    The node runs $(0,1)\leftarrow \text{CheckProof}(pk,chal,pf)$ to check the correctness of the proof. That is, if Equation (\ref{equ:v}) holds, the proof passes and returns $1$; otherwise, the proof fails and returns $0$.
    \begin{equation}
        \label{equ:v}
    e(\sigma,g)=e(\prod_{(i,v_{i})\in chal}H_{1}(i)^{v_{i}}\cdot u^{\mu},\beta)
    \end{equation}
\end{itemize}


\subsection{Proof-of-Work with Low-Variance Mining}
Classical PoW\cite{2008bitcoin} adopts the operation mode that a single node reaches the standard, and the block interval is controlled by the global difficulty. Although the block interval can guarantee the stability of the average value on the macro level, there is a high variance between the block intervals of different rounds on the micro level, whereas the time dimension in our protocol is measured based on the block height, and as the unfairness example mentioned earlier, the scheme with low variance of the block intervals is exactly the solution that we need. As mentioned in the unfairness example, a solution with low variance between blocks is exactly what we need. Thus, we shift the blocking criterion from a single node measure to a global node measure, and shift the difficulty constraint from a single node computation to the average of a number of optimal node computations globally, which can significantly reduce the block interval variance\cite{2020bobtail} by a corresponding margin.


\begin{figure}[htbp]
    \centering
    \fbox{
        \begin{minipage}[c][8cm][c]{0.9\columnwidth}
            \centering
            
            \vspace*{\fill}
            \textit{Nakamoto vs. Bobtail}
            \vspace*{\fill}
        \end{minipage}
    }
    \caption{Distributed PDP Scheme}
    \label{fig:Bobtail} 
\end{figure}



Suppose that over a period of nodes trying to mine over time, $h$ hashes, $H_1, H_2, \dots, H_h$, are generated by miners across the network. Sort them from smallest to largest to obtain the order statistic $V_i = H_{(i)}$. The mining statistic $W_k$ for this scheme is defined as the average of the first $k$ lowest hashes:
\begin{equation}
W_k = \frac{1}{k}\sum_{i=1}^{k}V_i
\end{equation}
A new block is discovered when one of the realized values $w_k$ of $W_k$ is less than or equal to the target value $t_k$, i.e., when the following conditions are met:
\begin{equation}
w_k \le t_k
\end{equation}
The most important effect of this scheme is a significant reduction in the variance of the block-out time. It can be shown that when the mining statistic $W_k$ is used instead of the traditional $W_1$, the ratio of the variance of the block-out time $Var[Y_k]$ to the variance of the block-out time $Var[Y_1]$ when $k=1$, while keeping the desired block-out time fixed, is:
\begin{equation}
\frac{Var[Y_k]}{Var[Y_1]} = \frac{8k+4}{6(k^2+k)} = O(\frac{1}{k})
\end{equation}
The variance of the blocking time decreases at a rate of $O(1/k)$ as $k$ increases. For example, when $k \ge 40$, the variance can be reduced to less than 1\% of the original.


\subsection{Incrementally-Variable Computation with Folding Scheme}
In our BPoSt, the entire evidence chain is physically dispersed within each block. If we want users to verify whether the storage is executed correctly, the traditional method is to traverse the blocks to collect evidence or let the storage nodes collect evidence and consolidate it, and both of these ideas will cause a huge computational overhead for either side of the protocol. Especially the latter, previous research has used ZK\_SNARKs\cite{2023epost, 2019pospacetime} technique to aggregate evidence for user verification, but this scheme is often accompanied by complex idiosyncratic circuit design and high overhead, which is very inappropriate for storage-intensive services.


Therefore, we adopt a kind of "folding"\cite{2022nova} idea in BPoSt, by introducing \textit{Incrementally Verifiable Computation}\cite{2008ivc}with some reduction methods to prove the whole chain of evidence. Correctness. An obvious feature of the chain of evidence is that the proofs are generated round by round, so we thought of using IVC to generate the proofs iteratively, spreading the workload to each round of proofs. However, traditional IVC relies on embedding a full SNARK verifier in the recursive loop, but this leads to a huge recursive overhead \cite{2020proof, 2021proofs}, so we use the task of verifying that \textbf{two}independent NP instances are satisfiable, reduce to verify that \ textbf{one} new, merged instance as a task of verifying the satisfiability of \textbf{one} new, merged instance\cite{2022nova}. 


\subsubsection{Definition of NIFS}
The scheme consists of a set of polynomial-time algorithms $\text{NIFS}(\mathcal{G}, \mathcal{K}, \mathcal{P}, \mathcal{V})$ defining the protocol life cycle. The details of this scheme are as follows.
\begin{itemize}
    \item \textbf{$\mathcal{G}(1^{\lambda}) \rightarrow pp$} is run once by the protocol designer to generate the global public parameter $pp$.
    \item \textbf{$\mathcal{K}(pp, F) \rightarrow (pk, vk)$} generates a proof key $pk$ and a verification key $vk$ for a specific single-step computational logic $F$. 
    \item \textbf{$\mathcal{P}(pk, (i, z_0, z_i), \omega_i, \Pi_{i-1}) \rightarrow \Pi_i$:} is an iterative process that is run by the storage node at a height of $i>0$ per block.
    \item \textbf{$\mathcal{P}_{zk}(pk, (l, z_0, z_l), \Pi_l) \rightarrow \pi_{final}$} is run by the storage node when it needs to generate a short proof of the storage for the first $l$ blocks.
    \item \textbf{$\mathcal{V}_{zk}(vk, (l, z_0, z_l), \pi_{final}) \rightarrow \{0,1\}$} Any end-user or blockchain node can run this efficient verification algorithm.
\end{itemize}


\subsubsection{IVC with Folding}
Combining the Distributed PDP above and the actual data structure in BPoSt, we adapt the original NIFS to the following flow.


\textbf{Setup \& Key Generate:} The protocol first runs $\mathcal{G}(1^{\lambda}$ and $\mathcal{K}(pp, F)$, generating the parameters necessary for the subsequent steps, where, the $F$ in our protocol represents the R1CS representation of a single round of stored verification logic. And the core of the $\text{NIFS}. \mathcal{K}$ is the generation of keys for the "augmented function" $F'$, which contains not only $F$ but also recursive logic.


\textbf{Iterative Proof Generation: }
The storage node runs the prover algorithm $P$ at each block height $i$ to update its IVC proofs. An IVC proof $\Pi_i$ at step $i$ is a tuple containing the accumulated instance-evidence pairs $(U_i, W_i)$ and the instance-evidence pairs $(u_i, w_i)$ at the current step.
\begin{itemize}
    \item 
    Prover calls the `$\text{NIFS.}\mathcal{P}$`, inputs the cumulative instances $(U_{i-1}, W_{i-1})$ from the previous round and the computed instances $(u_{i-1}, w_{i-1})$ from the previous step, and collapses them into a new cumulative-instance-evidence pair $(U_i, W_i)$.
    \begin{equation}
    (U_i, W_i) \leftarrow \text{NIFS.}\mathcal{P}(pk, (U_{i-1}, W_{i-1}), (u_{i-1}, w_{i-1}))
    \end{equation}
    \item 
    Prover executes the logic of the "augmentation function" $F'$ to generate a new, satisfiable relaxed R1CS instance-evidence pair $(u_i, w_i)$ for the current step of the computation.
\end{itemize}


\textbf{Proof Compression and Final Verification: } 
Although IVC proves that the size of $\Pi_i$ is constant, the evidence it contains, $W_i, w_i$, is still large enough that a final compression step is needed to obtain a further, shorter proof, and the single-step operation here is implemented using the associated SNARK.
\begin{itemize}
    \item 
    Prover first uses `$\text{NIFS.}\mathcal{P}$` to fold the two instance-evidence pairs in $\Pi_l$ one last time, yielding a final, fully cumulative instance-evidence pair $(U', W')$.
    \begin{equation}
    (U', W') \leftarrow \text{NIFS.}\mathcal{P}(pk, (U_l, W_l), (u_l, w_l))
    \end{equation}
    \item 
    $\pi_{final}$The prover generates a short, zero-knowledge proof $\pi_{final}$ for instance $U'$ and proof $W'$ using a proof algorithm customized for ZK-SNARK for relaxed R1CS.
    \item 
    Now, it turns to Verifiers. Verifier runs `$\text{NIFS.}\mathcal{V}$` to recompute the final collapsed instance $U'$ using the publicly available instance parts $U_l, u_l$ of the proof and the collapsed cross-item commitment $\overline{T}$.
    \begin{equation}
    U' \leftarrow \text{NIFS.}\mathcal{V}(vk, U_l, u_l, \overline{T})
    \end{equation}
    Then Verifier runs ZK-SNARK's verification algorithm to check whether $\pi_{final}$ is a valid proof for instance $U'$.
\end{itemize}


\section{BPoSt Consensus Protocol}
After describing the key components of our BPoSt above, we can now finally start to build the protocol based on them. We start with the core Proof of Storage-Time, which illustrates how PoSt will actually behave in this protocol, then we build security assumptions and definitions that address the fundamentals of the protocol, and finally, we give the complete flow of the protocol.


\subsection{Proof of Storage-Time}
Literally, Proof of Storage-Time consists of two directions, Storage and Time. Proving and verifying that a node has correctly stored specific data in a specific period of time is the core meaning of PoSt. Let's look at the two directions separately.


\subsubsection{Proof of Storage}
BPoSt's Proof of Storage is implemented by the Distributed PDP mentioned above, for a storage requirement, i.e., a user request to store a file. First, the protocol requires that the file be chunked, and all proofs in a round are in chunks. Each round, the storage node needs to process a "specific" chunk of the file within the demand to generate the corresponding PDP proof. The "specific" is randomly selected to avoid the huge computational overhead and proof size associated with global verification. 


This random selection is performed by a pseudo-random generator, where the random seed uses the block header hash value of the last generated block, which reduces the randomness to a hash function, and utilizes its random distribution to ensure the randomness of the selection. But then someone may ask: Randomly selecting a certain number of blocks will lead to dishonest storage nodes being 'lazy' and not be detected? The answer is no, thanks to the probabilistic distribution of the hash function, the verification action will definitely detect the misbehavior with a high probability within a finite (acceptable) rounds \cite{2023epost}.


\subsubsection{Proof of Time}
For Proof of Time, the chain structure of the blockchain is undoubtedly a good time anchor, we naturally choose the block height to measure the storage time, but also to circumvent the existing program, the need to store the node continues to calculate, to prove that they continue to store the program, i.e., the use of Verifiable Delay Functions\cite{2018vdf} to calculate the time of the idea of the \cite{2019pospacetime, 2023epost}. Traditional time measurement, to a certain extent, is more deterministic, figuratively speaking, the frequency of the "second hand ticking" is stable, but also has a strong security guarantee, the underlying mathematical problems from the principle of the node to greatly increase the difficulty of cheating, but we believe that this approach is not "elegant" and the storage node. But we think that this way is not "elegant" and the continuous calculation of the storage node is a kind of "useless work", as a consensus protocol, why not naturally use the blockchain way to solve the problem?


We then use the block height to measure time, but as mentioned above, when describing our choice of a low variance PoW scheme\cite{2020bobtail}, the classical PoW protocol is flawed for our application scenario. The problem of unequal actual storage times mentioned above, in addition to leading to uneven node overhead, can also lead to storage nodes trying to manipulate the time in order to maximize their own benefits. Therefore, we adopt a low variance scheme to mitigate the threat of time manipulation on one hand, and on the other hand, to make our "ticking" closer to the actual time passing. In this way, PoW can be used as a better alternative to VDF under safe conditions.


~


In this way, we have constructed a feasible proof idea, using the PDP proof of a file block in combination with the block height to measure Storage Time. However, upon closer examination, the actual situation may be more complicated. The storage capacity of a storage node is generally very large, allowing it to store a large number of files from various sources. How can we represent such storage? How to represent this storage, and how to organize the proofs that will be accumulated as the storage progresses. All these problems pose challenges to the data structure we want to build. To solve this problem, we choose to build Multidimensional State Trees based on the Merkle tree in blockchain.


\subsubsection{Multidimensional State Trees}
Our Multidimensional State Trees are divided into two dimensions, time and storage, both from the perspective of a particular storage node. The temporal state tree refers to the state of a file throughout its storage, while the storage state tree refers to the state of all files stored at that node at a given time. The former is used to realize the public verifiability of the protocol, and the latter is used as one of the weights for nodes to compete for the leader.
\begin{figure}[htbp]
    \centering
    \fbox{
        \begin{minipage}[c][8cm][c]{0.9\columnwidth}
            \centering
            
            \vspace*{\fill}
            \textit{A. Time B. Storage}
            \vspace*{\fill}
        \end{minipage}
    }
    \caption{Multidimensional State Trees}
    \label{fig:tree} 
\end{figure}

\begin{itemize}
    \item \textbf{Time state tree: }
    As mentioned above, we require the user to initialize the file when uploading the file. In addition to the chunking mentioned above, the user must also provide the initial parameters so that the storage node can use the distributed PDP signature to generate the initial proofs for all the chunks of the file and construct an initial state tree, fixing the proofs as the initial commitment of the file storage. In each subsequent round, the storage node needs to select a specific block to prove, and these proofs need to be attached to the corresponding nodes in the tree, thus generating a state tree that records all the proofs as the storage progresses. That is, each round of PDP proofs is an update of the file state in the time dimension, which is what needs to be included in the blocking, but even if there is no contention for blocking in the current round, the node must update the state tree.
    \item \textbf{Storage state tree: }
    A node may store multiple files, so how to measure the current weight of the node becomes a problem. Our solution is to synthesize the root of the temporal state tree of all the files that are still stored in the current round of the node into another storage state tree, and when competing for the leader, according to the number of leaves in the state tree, nodes are allowed to generate the corresponding number of "lots" in parallel(can also be generated in real time as a documented proof is completed), and select the optimal result of the specific number of lots after de-emphasis to compete for the leader. We encourage weak nodes to store small files. We encourage weak nodes to store small files to improve their competitiveness, and at the same time give more weight to strong nodes to improve their resource utilization, so as to curb the trend of centralization.
\end{itemize}


\subsection{Definitions of Safety and Liveness}
According to the definition of properties of consensus protocols, which usually adopts the definition method proposed by Lamport \cite{1977lamport}, we divide the properties of the protocol into two parts: Liveness (L) and Safety (S), which together define the correctness of the protocol execution. Safety is mainly concerned with the fact that certain undesirable events do not occur and do not enter an error state. Liveness, on the other hand, is concerned with the fact that certain benign events will eventually occur and the system will not remain in some kind of stagnant state indefinitely. This is described below, and the formal definitions are given in Section \ref{sec:formal}.
\begin{itemize}
    \item \textbf{Agreement(S): }
    Distributed nodes have one and only one decision at the end of processing that is accepted by all nodes.
    \item \textbf{Termination(L): }
    All non-faulty nodes will eventually make a decision, and all processes will reach a consensus within a finite period of time. (Derived from: all correct state trees are eventually uploaded to the chain).
    \item \textbf{Finalization(L): }
    Confirmed blocks cannot be altered, and uploaded proofs cannot be rolled back.
\end{itemize}


\subsubsection{Roles}
Based on the above definition of participant roles in distributed PDP, we separate the roles in the whole protocol in this way. The user is only involved in the authentication process, while the storage nodes need to be potentially involved in proof, consensus, and verification. This gives us three roles in the protocol: Leader, Provider, and Observer.
\begin{itemize}
    \item \textbf{Leader: }
    The Leader is selected through a PoW consensus process. When the average of the optimal solutions of a fixed number of uploaded lots reaches the criterion, it can start to select the Leader, and the node that holds the optimal lot will be selected as the Leader. It needs to collect the information of all the nodes (i.e., Providers) holding the optimal solutions of a fixed number of lots that are participating in the competition, to verify whether the proofs are valid or not, and construct the valid part into a new block to get a part of the proceeds from these transactions. It needs to collect information from all the competing nodes (i.e., Providers) with a fixed number of optimal lots, verify the validity of these proofs, construct a new block with the valid ones, and get a portion of the proceeds from these transactions.
    \item \textbf{Provider: }
    If a node submits its lot and is not selected as a Leader but is ranked within the required number, it will be selected as a Provider. At this point, it needs to submit the information it needs to upload, i.e., the state tree it has completed in the current round, to the Leader for verification of uploadability.
    \item \textbf{Observer: }
    If a storage node is neither a Leader nor a Provider, it becomes an Observer, like a user. This group can utilize the publicly verifiable nature of the proofs on the chain and can submit proofs of cheating to various parties to receive certain rewards.
\end{itemize}


\subsection{The BPoSt Consensus Protocol}
Here we start to introduce the basic flow of the protocol, starting from the user's submission of a storage request, and ending with the generation of a short proof of storage.


\begin{figure}[htbp]
    \centering
    \fbox{
        \begin{minipage}[c][8cm][c]{0.9\columnwidth}
            \centering
            \vspace*{\fill}
            \textit{BPoSt}
            \vspace*{\fill}
        \end{minipage}
    }
    \caption{BPoSt work flow}
    \label{fig:BPoSt} 
\end{figure}


\subsubsection{Pre-Prepare}
The user selects the file $F$ and runs $\text{PDP}$, which is chunked into a series of file blocks $\{b_1, b_2, ... , b_n\}$, and to generate a series of signatures $\{\sigma_{1}, \sigma_{2}, ... , \sigma_{i}\}$, followed by a set of necessary parameters as well as a collection of signatures $T_{F}=\{\sigma_{i}\}_{1\le i\le m}$ for the file $F$ and a collection of blocks $B=\{b_{i}\}_{1\le i\le m}$ for the file to be published. The storage node accepts it and builds the corresponding initial commitment for the file, and calls $\text{NIFS.}\mathcal{K}$ to generate the parameters needed for folding, thus starting the procedure.


\subsubsection{Storage and Proof}
Formally entering the storage phase, each round of storage nodes needs to generate corresponding proofs based on a specific set of challenges $Chal$, where $Chal_i = Rand(H(Block_{i-1})) \cup Chal_{u}$, and $Chal_{u}$ is a user-specified challenge. The corresponding proof is appended to the corresponding leaf of the state tree. Subsequently, after all the challenges have completed their responses, compute and submit their own set of lots $lots_i = \{lots_{i_{j}}\}_{1\le i\le f}$, with $f$ being the number of files stored by the nodes in the current round, to compete for the leader. Note that you also need to call $\text{NIFS.}\mathcal{P}$ to collapse the proof for use at the end of the process. When the nodes in the network detect the smallest $k$ $lots$ that satisfy 
\[
\frac{lots_{min}\{lots_{i}\}_{1\le i\le k}}{k} \le \text{dif}
\] 
And dif is the current difficulty, then the node that has the best $lots$ will be chosen as leader. The leader collects information about the nodes that have organized and verified the smallest $k$ $lots$, and then proceeds to the next round of competition for leader. The leader collects and verifies the information of the nodes corresponding to the $k$ smallest lots (after de-weighting), and then goes to the next round.


\subsubsection{Proof Folding and Verification}
The proof continues until the end of the procedure, at which point the storage node calls $\text{NIFS.}\mathcal{P}$ to collapse the proof one last time with a short zero-knowledge proof based on the collapsed proof in the state tree. At this point, the storage node completes all the work, and the user can continue to call $\text{NIFS.}\mathcal{V}$ to verify the correctness of the whole process.


\section{Formal Verification of BPoSt}
\label{sec:formal}

\subsection{Safety}


\subsection{Liveness}


\section{Evaluation}


\section{Related Work}

Similar work on primitive PoSt\cite{2019pospacetime}, cPoSt\cite{2020cpost}, ePoSt\cite{2023epost}, stoRNA\cite{2023storna}, etc., has all progressed in one way or another, but there are still some problems, for example


\section{Conclusion}

\nocite{*}
\bibliographystyle{ieeetr}
\bibliography{refs}


\end{document}
